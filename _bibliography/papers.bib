---
---

@article{yoo2023vosvfi,
    title     = {Video Object Segmentation-aware Video Frame Interpolation}, 
    author    = {Yoo, Jun-Sang and Lee, Hongjae and Jung, Seung-Won},
    journal   = {ICCV},
    year      = {2023},
    abstract  = {Video frame interpolation (VFI) is a very active research topic due to its broad applicability to many applications, including video enhancement, video encoding, and slow-motion effects. VFI methods have been advanced by improving the overall image quality for challenging sequences containing occlusions, large motion, and dynamic texture. This mainstream research direction neglects that foreground and background regions have different importance in perceptual image quality. Moreover, accurate synthesis of moving objects can be of utmost importance in computer vision applications. In this paper, we propose a video object segmentation (VOS)-aware training framework called VOS-VFI that allows VFI models to interpolate frames with more precise object boundaries. Specifically, we exploit VOS as an auxiliary task to help train VFI models by providing additional loss functions, including segmentation loss and bi-directional consistency loss. From extensive experiments, we demonstrate that VOS-VFI can boost the performance of existing VFI models by rendering clear object boundaries. Moreover, VOS-VFI displays its effectiveness on multiple benchmarks for different applications, including video object segmentation, object pose estimation, and visual tracking.},
    abbr      = {ICCV},
    selected  = {true},
    img_path  = {assets/img/vos_vfi.png}
}

@article{ko2023tits,
    title     = {Pose and Shape Estimation of Humans in Vehicles}, 
    author    = {Ko, Kwang-Lim and Yoo, Jun-Sang and Han, Chang-Woo and Kim, Jungyeop and Jung, Seung-Won},
    journal   = {IEEE Transactions on Intelligent Transportation Systems},
    year      = {2023},
    abstract  = {As autonomous driving technologies advance, occupants are expected to be free from driving, diversifying interaction scenarios with vehicles. Despite the growing importance of in-vehicle occupant monitoring systems, most existing systems focus on the face or head tracking of occupants, and only a few studies have attempted to detect their poses. In this paper, we present the first in-vehicle environment-specialized framework for the joint estimation of 3D human pose and shape from a single image. To this end, we introduce a new dataset called Human In VEhicles (HIVE), which contains a large collection of synthesized humans with different shapes and poses in vehicle images. HIVE provides RGB and NIR in-vehicle image pairs with ground-truth 2D and 3D pose and shape annotations, respectively. In addition, to exploit the different characteristics of humans in vehicles and unconstrained environments, we present a new pose prior penalizing poses that deviate from in-vehicle poses. The pose prior is derived using a variational autoencoder trained with in-vehicle human pose data. By using the proposed HIVE dataset and pose prior along with an elaborately designed two-stage training procedure, our method exhibits significantly improved pose and shape estimation performance compared with state-of-the-art methods for real-world test images captured in vehicles under different conditions.},
    abbr      = {IEEE Transactions on Intelligent Transportation Systems},
    selected  = {true},
    img_path  = {assets/img/pse.png}
}

@article{yoo2022hst,
    title     = {Hierarchical Spatiotemporal Transformers for Video Object Segmentation}, 
    author    = {Yoo, Jun-Sang and Lee, Hongjae and Jung, Seung-Won},
    journal   = {ICCVW},
    year      = {2023},
    abstract  = {This paper presents a novel framework called HST for semi-supervised video object segmentation (VOS). HST extracts image and video features using the latest Swin Transformer and Video Swin Transformer to inherit their inductive bias for the spatiotemporal locality, which is essential for temporally coherent VOS. To take full advantage of the image and video features, HST casts image and video features as a query and memory, respectively. By applying efficient memory read operations at multiple scales, HST produces hierarchical features for the precise reconstruction of object masks. HST shows effectiveness and robustness in handling challenging scenarios with occluded and fast-moving objects under cluttered backgrounds. In particular, HST-B outperforms the state-of-the-art competitors on multiple popular benchmarks.},
    abbr      = {ICCVW},
    selected  = {true},
    arxiv     = {2307.08263},
    img_path  = {assets/img/hst.png}
}

@article{lee2022gpsglass,
    title     = {GPS-GLASS: Learning Nighttime Semantic Segmentation Using Daytime Video and GPS data}, 
    author    = {Lee, Hongjae and Han, Changwoo and Yoo, Jun-Sang and Jung, Seung-Won},
    journal   = {ICCVW},
    year      = {2023},
    abstract  = {Semantic segmentation for autonomous driving should be robust against various in-the-wild environments. Nighttime semantic segmentation is especially challenging due to a lack of annotated nighttime images and a large domain gap from daytime images with sufficient annotation. In this paper, we propose a novel GPS-based training framework for nighttime semantic segmentation. Given GPS-aligned pairs of daytime and nighttime images, we perform cross-domain correspondence matching to obtain pixel-level pseudo supervision. Moreover, we conduct flow estimation between daytime video frames and apply GPS-based scaling to acquire another pixel-level pseudo supervision. Using these pseudo supervisions with a confidence map, we train a nighttime semantic segmentation network without any annotation from nighttime images. Experimental results demonstrate the effectiveness of the proposed method on several nighttime semantic segmentation datasets.},
    abbr      = {ICCVW},
    code      = {https://github.com/jimmy9704/GPS-GLASS},
    selected  = {true},
    arxiv     = {2207.13297},
    img_path  = {assets/img/gps_glass.png}
}

@article{yoo2022rzsr,
    title     = {RZSR: Reference-based Zero-Shot Super-Resolution with Depth Guided Self-Exemplars}, 
    author    = {Yoo, Jun-Sang and Kim, Dong-Wook and Lu, Yucheng and Jung, Seung-Won},
    journal   = {IEEE Transaction on Multimedia (TMM)},
    year      = {2022},
    abstract  = {Recent methods for single image super-resolution (SISR) have demonstrated outstanding performance in generating high-resolution (HR) images from low-resolution (LR) images. However, most of these methods show their superiority using synthetically generated LR images, and their generalizability to real-world images is often not satisfactory. In this paper, we pay attention to two well-known strategies developed for robust super-resolution (SR), i.e., reference-based SR (RefSR) and zero-shot SR (ZSSR), and propose an integrated solution, called reference-based zero-shot SR (RZSR). Following the principle of ZSSR, we train an image-specific SR network at test time using training samples extracted only from the input image itself. To advance ZSSR, we obtain reference image patches with rich textures and high-frequency details which are also extracted only from the input image using cross-scale matching. To this end, we construct an internal reference dataset and retrieve reference image patches from the dataset using depth information. Using LR patches and their corresponding HR reference patches, we train a RefSR network that is embodied with a non-local attention module. Experimental results demonstrate the superiority of the proposed RZSR compared to the previous ZSSR methods and robustness to unseen images compared to other fully supervised SISR methods. },
    abbr      = {IEEE Transaction on Multimedia (TMM)},
    code      = {https://github.com/junsang7777/RZSR-Reference-based-Zero-Shot-Super-Resolution-with-Depth-Guided-Self-Exemplars},
    selected  = {true},
    arxiv     = {2208.11313},
    img_path  = {assets/img/tmm.png}
}
