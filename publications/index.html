<!DOCTYPE html> <html lang="en"> <head> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title>Publications | Jun-Sang Yoo</title> <meta name="author" content="Jun-Sang Yoo"/> <meta name="description" content="A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. "/> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"/> <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"/> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.4/css/all.min.css" integrity="sha256-mUZM63G8m73Mcidfrv5E+Y61y7a12O5mW4ezU3bxqW4=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/github.css" media="none" id="highlight_theme_light"/> <link rel="shortcut icon" href="/assets/img/%F0%9F%A7%91%F0%9F%8F%BB%E2%80%8D%F0%9F%8E%93"/> <link rel="stylesheet" href="/assets/css/main.css"> <link rel="canonical" href="https://jimmy9704.github.io/publications/"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/native.css" media="none" id="highlight_theme_dark"/> <script src="/assets/js/theme.js"></script> <script src="/assets/js/dark_mode.js"></script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="https://jimmy9704.github.io/"><span class="font-weight-bold">Jun-Sang</span> Yoo</a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">About</a> </li> <li class="nav-item active"> <a class="nav-link" href="/publications/">Publications<span class="sr-only">(current)</span></a> </li> <div class="toggle-container"> <a id="light-toggle"> <i class="fas fa-moon"></i> <i class="fas fa-sun"></i> </a> </div> </ul> </div> </div> </nav> </header> <div class="container mt-5"> <div class="post"> <header class="post-header"> <h1 class="post-title">Publications</h1> <p class="post-description"></p> </header> <article> <div class="publications"> <h2 class="year">2024</h2> <ol class="bibliography"><li> <div class="row"> <div class="col-sm-3"> <figure> <picture> <source media="(max-width: 480px)" srcset="/assets/img/refqsr-480.webp"></source> <source media="(max-width: 800px)" srcset="/assets/img/refqsr-800.webp"></source> <source media="(max-width: 1400px)" srcset="/assets/img/refqsr-1400.webp"></source> <img class="img-fluid rounded z-depth-1" src="/assets/img/refqsr.png" data-zoomable=""> </picture> </figure> </div> <div id="lee2024refqsr" class="col-sm-8"> <div class="title">RefQSR: Reference-based Quantization for Image Super-Resolution Networks</div> <div class="author">Lee, Hongjae,  <em>Yoo, Jun-Sang</em>, and Jung, Seung-Won </div> <div class="periodical"> <em>IEEE TIP</em> 2024 </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/2404.01690" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">arXiv</a> <a href="https://jimmy9704.github.io/RefQSR/" class="btn btn-sm z-depth-0" role="button">HTML</a> </div> <div class="abstract hidden"> <p>Single image super-resolution (SISR) aims to reconstruct a high-resolution image from its low-resolution observation. Recent deep learning-based SISR models show high performance at the expense of increased computational costs, limiting their use in resource-constrained environments. As a promising solution for computationally efficient network design, network quantization has been extensively studied. However, existing quantization methods developed for SISR have yet to effectively exploit image self-similarity, which is a new direction for exploration in this study. We introduce a novel method called reference-based quantization for image super-resolution (RefQSR) that applies high-bit quantization to several representative patches and uses them as references for low-bit quantization of the rest of the patches in an image. To this end, we design dedicated patch clustering and reference-based quantization modules and integrate them into existing SISR network quantization methods. The experimental results demonstrate the effectiveness of RefQSR on various SISR networks and quantization methods.</p> </div> </div> </div> </li></ol> <h2 class="year">2023</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col-sm-3"> <figure> <picture> <source media="(max-width: 480px)" srcset="/assets/img/vos_vfi-480.webp"></source> <source media="(max-width: 800px)" srcset="/assets/img/vos_vfi-800.webp"></source> <source media="(max-width: 1400px)" srcset="/assets/img/vos_vfi-1400.webp"></source> <img class="img-fluid rounded z-depth-1" src="/assets/img/vos_vfi.png" data-zoomable=""> </picture> </figure> </div> <div id="yoo2023vosvfi" class="col-sm-8"> <div class="title">Video Object Segmentation-aware Video Frame Interpolation</div> <div class="author"> <em>Yoo, Jun-Sang</em>, Lee, Hongjae, and Jung, Seung-Won </div> <div class="periodical"> <em>ICCV</em> 2023 </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> </div> <div class="abstract hidden"> <p>Video frame interpolation (VFI) is a very active research topic due to its broad applicability to many applications, including video enhancement, video encoding, and slow-motion effects. VFI methods have been advanced by improving the overall image quality for challenging sequences containing occlusions, large motion, and dynamic texture. This mainstream research direction neglects that foreground and background regions have different importance in perceptual image quality. Moreover, accurate synthesis of moving objects can be of utmost importance in computer vision applications. In this paper, we propose a video object segmentation (VOS)-aware training framework called VOS-VFI that allows VFI models to interpolate frames with more precise object boundaries. Specifically, we exploit VOS as an auxiliary task to help train VFI models by providing additional loss functions, including segmentation loss and bi-directional consistency loss. From extensive experiments, we demonstrate that VOS-VFI can boost the performance of existing VFI models by rendering clear object boundaries. Moreover, VOS-VFI displays its effectiveness on multiple benchmarks for different applications, including video object segmentation, object pose estimation, and visual tracking.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-3"> <figure> <picture> <source media="(max-width: 480px)" srcset="/assets/img/pse-480.webp"></source> <source media="(max-width: 800px)" srcset="/assets/img/pse-800.webp"></source> <source media="(max-width: 1400px)" srcset="/assets/img/pse-1400.webp"></source> <img class="img-fluid rounded z-depth-1" src="/assets/img/pse.png" data-zoomable=""> </picture> </figure> </div> <div id="ko2023tits" class="col-sm-8"> <div class="title">Pose and Shape Estimation of Humans in Vehicles</div> <div class="author">Ko, Kwang-Lim,  <em>Yoo, Jun-Sang</em>, Han, Chang-Woo, Kim, Jungyeop, and Jung, Seung-Won </div> <div class="periodical"> <em>IEEE Transactions on Intelligent Transportation Systems</em> 2023 </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> </div> <div class="abstract hidden"> <p>As autonomous driving technologies advance, occupants are expected to be free from driving, diversifying interaction scenarios with vehicles. Despite the growing importance of in-vehicle occupant monitoring systems, most existing systems focus on the face or head tracking of occupants, and only a few studies have attempted to detect their poses. In this paper, we present the first in-vehicle environment-specialized framework for the joint estimation of 3D human pose and shape from a single image. To this end, we introduce a new dataset called Human In VEhicles (HIVE), which contains a large collection of synthesized humans with different shapes and poses in vehicle images. HIVE provides RGB and NIR in-vehicle image pairs with ground-truth 2D and 3D pose and shape annotations, respectively. In addition, to exploit the different characteristics of humans in vehicles and unconstrained environments, we present a new pose prior penalizing poses that deviate from in-vehicle poses. The pose prior is derived using a variational autoencoder trained with in-vehicle human pose data. By using the proposed HIVE dataset and pose prior along with an elaborately designed two-stage training procedure, our method exhibits significantly improved pose and shape estimation performance compared with state-of-the-art methods for real-world test images captured in vehicles under different conditions.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-3"> <figure> <picture> <source media="(max-width: 480px)" srcset="/assets/img/hst-480.webp"></source> <source media="(max-width: 800px)" srcset="/assets/img/hst-800.webp"></source> <source media="(max-width: 1400px)" srcset="/assets/img/hst-1400.webp"></source> <img class="img-fluid rounded z-depth-1" src="/assets/img/hst.png" data-zoomable=""> </picture> </figure> </div> <div id="yoo2022hst" class="col-sm-8"> <div class="title">Hierarchical Spatiotemporal Transformers for Video Object Segmentation</div> <div class="author"> <em>Yoo, Jun-Sang</em>, Lee, Hongjae, and Jung, Seung-Won </div> <div class="periodical"> <em>ICCVW</em> 2023 </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/2307.08263" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">arXiv</a> </div> <div class="abstract hidden"> <p>This paper presents a novel framework called HST for semi-supervised video object segmentation (VOS). HST extracts image and video features using the latest Swin Transformer and Video Swin Transformer to inherit their inductive bias for the spatiotemporal locality, which is essential for temporally coherent VOS. To take full advantage of the image and video features, HST casts image and video features as a query and memory, respectively. By applying efficient memory read operations at multiple scales, HST produces hierarchical features for the precise reconstruction of object masks. HST shows effectiveness and robustness in handling challenging scenarios with occluded and fast-moving objects under cluttered backgrounds. In particular, HST-B outperforms the state-of-the-art competitors on multiple popular benchmarks.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-3"> <figure> <picture> <source media="(max-width: 480px)" srcset="/assets/img/gps_glass-480.webp"></source> <source media="(max-width: 800px)" srcset="/assets/img/gps_glass-800.webp"></source> <source media="(max-width: 1400px)" srcset="/assets/img/gps_glass-1400.webp"></source> <img class="img-fluid rounded z-depth-1" src="/assets/img/gps_glass.png" data-zoomable=""> </picture> </figure> </div> <div id="lee2022gpsglass" class="col-sm-8"> <div class="title">GPS-GLASS: Learning Nighttime Semantic Segmentation Using Daytime Video and GPS data</div> <div class="author">Lee, Hongjae, Han, Changwoo,  <em>Yoo, Jun-Sang</em>, and Jung, Seung-Won </div> <div class="periodical"> <em>ICCVW</em> 2023 </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/2207.13297" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">arXiv</a> <a href="https://github.com/jimmy9704/GPS-GLASS" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Code</a> </div> <div class="abstract hidden"> <p>Semantic segmentation for autonomous driving should be robust against various in-the-wild environments. Nighttime semantic segmentation is especially challenging due to a lack of annotated nighttime images and a large domain gap from daytime images with sufficient annotation. In this paper, we propose a novel GPS-based training framework for nighttime semantic segmentation. Given GPS-aligned pairs of daytime and nighttime images, we perform cross-domain correspondence matching to obtain pixel-level pseudo supervision. Moreover, we conduct flow estimation between daytime video frames and apply GPS-based scaling to acquire another pixel-level pseudo supervision. Using these pseudo supervisions with a confidence map, we train a nighttime semantic segmentation network without any annotation from nighttime images. Experimental results demonstrate the effectiveness of the proposed method on several nighttime semantic segmentation datasets.</p> </div> </div> </div> </li> </ol> <h2 class="year">2022</h2> <ol class="bibliography"><li> <div class="row"> <div class="col-sm-3"> <figure> <picture> <source media="(max-width: 480px)" srcset="/assets/img/tmm-480.webp"></source> <source media="(max-width: 800px)" srcset="/assets/img/tmm-800.webp"></source> <source media="(max-width: 1400px)" srcset="/assets/img/tmm-1400.webp"></source> <img class="img-fluid rounded z-depth-1" src="/assets/img/tmm.png" data-zoomable=""> </picture> </figure> </div> <div id="yoo2022rzsr" class="col-sm-8"> <div class="title">RZSR: Reference-based Zero-Shot Super-Resolution with Depth Guided Self-Exemplars</div> <div class="author"> <em>Yoo, Jun-Sang</em>, Kim, Dong-Wook, Lu, Yucheng, and Jung, Seung-Won </div> <div class="periodical"> <em>IEEE Transaction on Multimedia (TMM)</em> 2022 </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/2208.11313" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">arXiv</a> <a href="https://github.com/junsang7777/RZSR-Reference-based-Zero-Shot-Super-Resolution-with-Depth-Guided-Self-Exemplars" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Code</a> </div> <div class="abstract hidden"> <p>Recent methods for single image super-resolution (SISR) have demonstrated outstanding performance in generating high-resolution (HR) images from low-resolution (LR) images. However, most of these methods show their superiority using synthetically generated LR images, and their generalizability to real-world images is often not satisfactory. In this paper, we pay attention to two well-known strategies developed for robust super-resolution (SR), i.e., reference-based SR (RefSR) and zero-shot SR (ZSSR), and propose an integrated solution, called reference-based zero-shot SR (RZSR). Following the principle of ZSSR, we train an image-specific SR network at test time using training samples extracted only from the input image itself. To advance ZSSR, we obtain reference image patches with rich textures and high-frequency details which are also extracted only from the input image using cross-scale matching. To this end, we construct an internal reference dataset and retrieve reference image patches from the dataset using depth information. Using LR patches and their corresponding HR reference patches, we train a RefSR network that is embodied with a non-local attention module. Experimental results demonstrate the superiority of the proposed RZSR compared to the previous ZSSR methods and robustness to unseen images compared to other fully supervised SISR methods. </p> </div> </div> </div> </li></ol> <h2 class="year">2021</h2> <ol class="bibliography"></ol> </div> </article> </div> </div> <footer class="sticky-bottom"> <div class="container mt-0" align="center"> <a href="https://info.flagcounter.com/5btW" target="_blank" rel="noopener noreferrer"><img src="https://s01.flagcounter.com/count/5btW/bg_FFFFFF/txt_000000/border_CCCCCC/columns_2/maxflags_10/viewers_0/labels_1/pageviews_1/flags_0/percent_0/" alt="Flag Counter" border="0"></a>              <a href="https://info.flagcounter.com/e0mA" target="_blank" rel="noopener noreferrer"><img src="https://s11.flagcounter.com/map/e0mA/size_s/txt_000000/border_CCCCCC/pageviews_1/viewers_0/flags_0/" alt="Flag Counter" border="0"></a> </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/@popperjs/core@2.11.2/dist/umd/popper.min.js" integrity="sha256-l/1pMF/+J4TThfgARS6KwWrk/egwuVvhRzfLAMQ6Ds4=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.min.js" integrity="sha256-SyTu6CwrfOhaznYZPoolVw2rxoY7lKYKQvqbtqN93HI=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.6/dist/medium-zoom.min.js" integrity="sha256-EdPgYcPk/IIrw7FYeuJQexva49pVRZNmt3LculEr7zM=" crossorigin="anonymous"></script> <script src="/assets/js/zoom.js"></script> <script src="/assets/js/common.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> </body> </html>